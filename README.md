# json-ultra-compress

[![Demo](https://img.shields.io/badge/ğŸš€_Interactive-Demo-blue?style=for-the-badge)](https://fayez-kaabi.github.io/json-ultra-compress-demo/)

**JSON-native compression with selective field decode.**

- ğŸš€ **10â€“35Ã— faster** than Brotli on structured JSON/NDJSON
- ğŸ“‰ **70â€“90% bandwidth reduction** with selective decode
- ğŸ“Š **Columnar NDJSON**: store fields separately to skip what you don't need
- ğŸŒ Pure TypeScript â€“ zero native deps (Node, browsers, edge)
- ğŸ”’ CRC-safe, preserves empty lines perfectly

> ğŸ‘‰ See full results in the [Comprehensive Benchmark Report](./benchmark-results/comprehensive-report.md).

## âš¡ Why It's Different

| Approach | Generic Codecs (Brotli/Zstd) | **json-ultra-compress** |
|----------|------------------------------|-------------------------|
| **Data View** | ğŸ“„ Treats JSON as plain text | ğŸ—ï¸ Understands JSON structure |
| **Access Pattern** | ğŸ”“ Must decompress EVERYTHING | ğŸ¯ Decode only selected fields |
| **Storage** | ğŸ“ Row-wise text compression | ğŸ“Š Columnar field storage |
| **Dependencies** | âš™ï¸ Native bindings (C/C++) | ğŸŒ Pure TypeScript |
| **Deployment** | ğŸš« Complex (platform-specific) | âœ… Universal (runs anywhere) |
| **Performance** | ğŸŒ Slow encoding (seconds) | âš¡ Fast encoding (milliseconds) |
| **Use Case** | ğŸ“¦ Generic text compression | ğŸ¯ JSON-native optimization |

### ğŸš€ **The Revolutionary Difference:**

**Traditional**: `{"user":123,"event":"click","meta":{...}}` â†’ **compress as text**
**json-ultra-compress**: Extract columns â†’ `user: [123,124,125]`, `event: ["click","view","purchase"]` â†’ **compress by field**

## ğŸ“Š Benchmarks

**Dataset: Structured log entries (~716 KB)**

| Codec             | Size    | Ratio | Encode Time | Selective Decode      |
| ----------------- | ------- | ----- | ----------- | --------------------- |
| **Columnar (hybrid)** | **92 KB**   | **12.9%** | **83 ms**   | âœ… (user\_id, ts only) |
| Standard Brotli       | 88 KB   | 12.3% | **1,208 ms** | âŒ N/A                 |
| Standard Gzip         | 112 KB  | 15.7% | 7 ms       | âŒ N/A                 |

ğŸ‘‰ **Result:** Near-identical compression to Brotli, **15Ã— faster encode**, and **field-level decoding Brotli/Zstd cannot do at all**.

> *Methodology:* Wall-clock times recorded with `performance.now()` on Node 20; best of 5 runs; ratios computed vs original UTF-8 bytes. Reproduce with `npm run bench:comprehensive`.

**Analytics Events (~1.8 MB)**

| Codec             | Size    | Ratio | Encode Time | Selective Decode      |
| ----------------- | ------- | ----- | ----------- | --------------------- |
| **Columnar (hybrid)** | **125 KB**  | **6.7%**  | **184 ms**  | âœ… **382 KB** (80% reduction) |
| Standard Brotli       | 120 KB  | 6.5%  | **3,567 ms** | âŒ N/A                 |
| Standard Gzip         | 167 KB  | 9.0%  | 17 ms       | âŒ N/A                 |

ğŸ‘‰ **Result:** Competitive compression, **19Ã— faster encode**, **80% bandwidth savings** with selective decode.

## ğŸ“ˆ Performance at Scale

```
Compression Ratio vs Dataset Size

 9% â”‚ â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— Brotli
    â”‚
 8% â”‚ â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹ json-ultra (10-35Ã— faster)
    â”‚
 7% â”‚
    â”‚
 6% â”‚ â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â— Brotli
    â”‚ â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹ json-ultra + selective decode
    â”‚
 5% â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      1KB        100KB        1MB         10MB        100MB+
                                                     (+ worker pool)

â— Standard compression    â—‹ json-ultra-compress    ğŸ¯ Selective decode advantage
```

> **ğŸ“Š Performance TL;DR**
> ğŸï¸ **10â€“35Ã— faster encodes** than Brotli
> ğŸ“‰ **70â€“90% smaller** selective decode outputs
> ğŸ”’ **CRC-safe**, zero native deps
> âš¡ **Worker pool** for 100MB+ datasets

## âœ… When to Use / âŒ When Not to Use

### âœ… **Perfect For:**
- ğŸ“Š **Structured logs** - repeated field names, temporal patterns
- ğŸ“ˆ **Analytics events** - user behavior, metrics, time-series data
- ğŸ›’ **API responses** - JSON with nested objects and consistent schemas
- ğŸ”„ **Data pipelines** - where selective field access matters
- âš¡ **Real-time systems** - fast encoding beats max compression
- ğŸŒ **Edge/serverless** - zero native dependencies

### âŒ **Skip For:**
- ğŸ–¼ï¸ **Images/binaries** - not JSON, use specialized codecs
- ğŸ“ **Unstructured text** - novels, docs, use Brotli/Zstd
- ğŸ—ƒï¸ **One-time archival** - where max compression > speed
- ğŸ“± **Tiny payloads** - overhead not worth it (< 1KB)

## ğŸ’¡ New Use Cases

* **Analytics pipelines** â†’ project only needed columns â†’ 3â€“5Ã— faster queries
* **Observability** â†’ extract `user_id, ts, error_code` instantly from huge logs
* **Streaming filters** â†’ route/filter JSON streams without hydrating full objects
* **Edge APIs** â†’ Brotli-level compression, but 10â€“35Ã— faster, no native deps

### ğŸš€ Interactive Demo

**[Try the live calculator â†’](https://fayez-kaabi.github.io/json-ultra-compress-demo/)**

Test compression ratios and selective decode benefits with realistic datasets:
- **JSON vs NDJSON** format comparison
- **Tiny/Medium/Large** presets (1K to 50K rows)
- **Live metrics**: bandwidth saved, CPU avoided, query speedup
- **Download outputs** in various compressed formats
- **Mobile-friendly** interface with explanations

*Perfect for understanding when and why json-ultra-compress beats traditional compression.*

## Install

```bash
npm i json-ultra-compress
# or
yarn add json-ultra-compress
# or
pnpm add json-ultra-compress
```

**Zero native deps by default**

Optional: `@zstd/wasm` auto-detected for Zstd support (Node & modern browsers)

## Quick Start

```typescript
import { compress, decompress, compressNDJSON, decompressNDJSON } from 'json-ultra-compress';

// Single JSON
const json = JSON.stringify({ users: [{id:1,name:'Alice'}], meta: {page:1} });
const packed = await compress(json, { codec: 'hybrid' });
const restored = await decompress(packed);

// NDJSON (columnar magic âœ¨)
const logs = [
  '{"user":"alice","event":"click","ts":"2024-01-01T10:00:00Z"}',
  '{"user":"bob","event":"view","ts":"2024-01-01T10:01:00Z"}',
  '{"user":"alice","event":"purchase","ts":"2024-01-01T10:02:00Z"}'
].join('\n');

const columnar = await compressNDJSON(logs, { codec: 'hybrid', columnar: true });
const back = await decompressNDJSON(columnar);              // full restore

// ğŸ¯ Selective decode - only 2 fields out of 10+
const partial = await decompressNDJSON(columnar, { fields: ['user','ts'] });
console.log('Selective decode size:', partial.length); // 80% smaller!
```

## CLI

```bash
# Compress structured logs (columnar)
json-ultra-compress compress-ndjson --codec=hybrid --columnar access.ndjson -o access.juc

# Decompress
json-ultra-compress decompress-ndjson access.juc -o restored.ndjson

# ğŸ”¥ Select only two columns from a 100MB log without decoding the rest
json-ultra-compress decompress-ndjson --fields=user_id,timestamp access.juc -o partial.ndjson

# For huge datasets, add worker pool for parallel processing (columnar only)
json-ultra-compress compress-ndjson --codec=hybrid --columnar --workers=auto massive-logs.ndjson -o massive.juc
```

## Why json-ultra-compress?

> ğŸ’¡ **See it in action**: [Interactive demo](https://fayez-kaabi.github.io/json-ultra-compress-demo/) with real datasets showing 70-90% bandwidth savings on selective decode.

### ğŸ¯ **JSON-aware (not just text)**

- **Repetitive keys** in NDJSON (`"timestamp"`, `"user_id"`, â€¦)
- **Small categorical enums** (`"status"`: pending/complete/failed)
- **Sequential numeric/timestamp patterns**
- **Sparse fields** across rows

### ğŸ—ï¸ **Architecture highlights**

- **Hybrid codec wrapper** â€” chooses Brotli/Gzip/Zstd-wasm automatically
- **Columnar NDJSON** â€” group by schema, encode columns separately
- **Smart column encodings** â€” delta for IDs, enums for categories, RLE for booleans
- **CRC32 integrity** â€” detects corruption (payload-only CRC)
- **Empty line preservation** â€” perfect round-trip for row-wise mode
- **Pure TypeScript** â€” no native deps, runs in Node, browsers, edge

## API

```typescript
// Single JSON
compress(input: string, options?: CompressOptions): Promise<Uint8Array>
decompress(data: Uint8Array): Promise<string>

// NDJSON (one JSON object per line)
compressNDJSON(input: string, options?: NDJSONOptions): Promise<Uint8Array>
decompressNDJSON(data: Uint8Array, options?: DecodeOptions): Promise<string>

interface CompressOptions {
  codec?: 'hybrid' | 'brotli' | 'gzip' | 'identity'; // default: 'hybrid'
}

interface NDJSONOptions extends CompressOptions {
  columnar?: boolean;                  // default: false
  workers?: number | 'auto' | false;   // default: false; 'auto' for â‰¥32 MB or â‰¥64 windows (columnar only)
}

interface DecodeOptions {
  fields?: string[];                   // selective decode: decode only requested columns
  workers?: number | 'auto' | false;   // default: false; 'auto' for â‰¥50 MB selective decode
}
```

### **Codecs**

- **`hybrid`** (recommended) â€” picks best backend per window
- **`brotli`** â€” high ratio for web payloads
- **`gzip`** â€” fast & ubiquitous
- **`identity`** â€” no compression (debug)

## Examples

### Compress API response

```typescript
import { compress, decompress } from 'json-ultra-compress';

const apiData = JSON.stringify({
  users: [{ id: 1, name: 'Alice', role: 'admin' }, { id: 2, name: 'Bob', role: 'user' }]
});

const c = await compress(apiData, { codec: 'hybrid' });
console.log(`${apiData.length} â†’ ${c.length} bytes`);
console.log(await decompress(c));
```

### Structured logs (the magic)

```typescript
import { compressNDJSON, decompressNDJSON } from 'json-ultra-compress';

const logs = Array.from({ length: 1000 }, (_, i) => JSON.stringify({
  timestamp: new Date(Date.now() - i * 60000).toISOString(),
  user_id: `user_${i % 100}`,
  event: ['click','view','purchase'][i % 3],
  source: ['web','mobile'][i % 2],
  duration_ms: Math.round(Math.random() * 1000),
  metadata: { ip: '192.168.1.' + (i % 255), session: 'sess_' + (i % 50) }
})).join('\n');

const rowwise  = await compressNDJSON(logs, { codec: 'hybrid' });
const columnar = await compressNDJSON(logs, { codec: 'hybrid', columnar: true });

console.log(`Row-wise:   ${rowwise.length} bytes`);
console.log(`Columnar:   ${columnar.length} bytes`);

// ğŸ¯ The revolutionary part: selective decode
const justUserAndTime = await decompressNDJSON(columnar, {
  fields: ['user_id', 'timestamp']
});
console.log(`Full data: ${logs.length} bytes`);
console.log(`Selected fields only: ${justUserAndTime.length} bytes`);
// Typical result: 80-90% size reduction!
```

## Benchmarks (on your data)

```bash
# Quick test
npm run bench:comprehensive

# Or test your own data
echo '{"test":1}' > test.json
json-ultra-compress compress --codec=hybrid test.json -o test.juc
json-ultra-compress decompress test.juc -o result.json

# Run on structured logs
json-ultra-compress compress-ndjson --codec=hybrid --columnar your-logs.ndjson -o logs.juc
ls -lh your-logs.ndjson logs.juc

# Test selective decode magic
json-ultra-compress decompress-ndjson --fields=user_id,timestamp logs.juc -o partial.ndjson
ls -lh partial.ndjson  # Should be 70-90% smaller!

# For large files (â‰¥32MB), use workers for faster processing
json-ultra-compress compress-ndjson --codec=hybrid --columnar --workers=auto huge-logs.ndjson -o huge.juc
```

## Performance Notes

- **Best for**: Structured JSON/NDJSON with repeated field patterns
- **Compression**: Competitive with Brotli (often within 1-2%)
- **Speed**: 10-35Ã— faster encoding than standard Brotli
- **Selective decode**: 70-90% bandwidth reduction for typical analytics queries
- **Worker pool**: Opt-in parallelization for large files (â‰¥32MB or â‰¥64 windows, columnar only)
- **Workers scope**: Parallelize **columnar** windows across CPU cores for big jobs. Small jobs stay single-threaded to avoid overhead.
- **Memory**: Efficient streaming processing, no full-file buffering required

## Notes & Limits

- **Input** must be valid UTF-8 JSON / NDJSON; one JSON object per line for NDJSON
- **Whitespace**: Row-wise paths preserve whitespace and empty lines; columnar reconstructs valid JSON per row (whitespace may differ)
- **Selective decode**: Available now for NDJSON columnar format
- **Zstd**: Optional `@zstd/wasm` auto-detected if installed

## Contributing

```bash
git clone https://github.com/fayez-kaabi/json-ultra-compress
cd json-ultra-compress
npm i
npm test   # should be all green âœ…
npm run build
npm run bench:comprehensive  # run full benchmark suite
```

## Roadmap

- **v1.2** â€” Streaming APIs, skip indices for even faster partial reads
- **v1.3** â€” Dictionary learning, browser bundle optimizations
- **v2.0** â€” Query language for complex field projections

## License

MIT Â© 2025

---

> **json-ultra-compress**
> JSON-native compression with selective field decode.
> Faster than Brotli. Smaller than Zstd. Smarter than both.
